I"F<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<p>自从 AlphaGo 出现以来，机器学习无疑是当今最火热的话题，而深度学习也成了机器学习领域内的热点，现在人工智能、大数据更是越来越贴近我们的日常生活，越来越多的人工智能应用开始在移植到移动端上，那能够快速高效地在移动端部署深度学习模型就变成亟待解决的问题了。<!-- more -->)</p>

<p>现阶段 app 上使用的深度学习主要有两种模式：</p>

<h2 id="online-方式">Online 方式</h2>

<p>首先在移动端做初步预处理，然后把数据传到服务器进行预测后返回移动端。</p>

<p>优点：部署相对简单，现成的框架（caffe，theano，mxnet，Torch) 做下封装就可以直接拿来用；使用服务器进行计算，性能强，能够处理比较大的模型</p>

<p>缺点：必须使用网络，而且展示效果依赖网速，不适合实时性要求高的应用。</p>

<h2 id="offline-方式">Offline 方式</h2>

<p>根据硬件的性能选择模型，在服务器训练得到模型，在手机上进行预测的过程。</p>

<p>优点：不需要使用网络，可以保护隐私</p>

<p>缺点：计算的性能、耗时等取决于移动端的性能，有些模型只能使用CPU，精度可能会有影响，无法进行类似云端的大规模分布式训练；移动端部署相对较麻烦，需要针对移动端进行优化；大模型耗费大量的资源（计算、内存、存储、电）。</p>

<h2 id="部署方法">部署方法</h2>

<p>下面主要介绍offline的部署方法。</p>

<p>主要分两个阶段，第一个阶段是训练并得到模型，第二个阶段则是在得到模型后，在移动端进行部署。本文主要讲解的为第二阶段。</p>

<h3 id="训练模型">训练模型</h3>

<p>在第一阶段训练模型中，已经有很成熟的开源框架和算法进行实现，但是为了能部署到移动端，还需要进行压缩加速。</p>

<h3 id="压缩网络">压缩网络</h3>

<p>目前深度学习在各个领域轻松碾压传统算法，不过真正用到实际项目中却会有很大的问题：</p>

<p>计算量非常巨大；</p>

<p>模型占用很高内存；</p>

<p>由于移动端系统资源有限，而深度学习模型可能会高达几百M，因此很难将深度学习应用到移动端系统中去。</p>

<h3 id="压缩方法">压缩方法</h3>

<p>综合现有的深度模型压缩方法，它们主要分为四类：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">方法名称</th>
      <th style="text-align: left">描述</th>
      <th style="text-align: center">应用场景</th>
      <th style="text-align: center">方法细节</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">参数修剪和共享（parameter pruning and sharing）</td>
      <td style="text-align: left">删除对准确率影响不大的参数</td>
      <td style="text-align: center">卷积层和全连接层</td>
      <td style="text-align: center">对不同设置具有鲁棒性，可以达到较好效果，支持从零训练和预训练</td>
    </tr>
    <tr>
      <td style="text-align: left">低秩因子分解（low-rank factorization）</td>
      <td style="text-align: left">使用矩阵对参数进行分解估计</td>
      <td style="text-align: center">卷积层和全连接层</td>
      <td style="text-align: center">标准化的途径，很容易实施，支持从零训练和预训练</td>
    </tr>
    <tr>
      <td style="text-align: left">转移/紧凑卷积滤波器（transferred/compact convolutional filters）</td>
      <td style="text-align: left">设计特别的卷积核来保存参数</td>
      <td style="text-align: center">只有卷积层</td>
      <td style="text-align: center">算法依赖于应用程序，通常可以取得好的表现，只能从零开始训练</td>
    </tr>
    <tr>
      <td style="text-align: left">知识蒸馏（knowledge distillation）</td>
      <td style="text-align: left">训练一个更紧凑的神经网络来从大的模型蒸馏知识</td>
      <td style="text-align: center">卷积层和全连接层</td>
      <td style="text-align: center">模型表现对应用程序和网络结构较为敏感，只能从零开始训练</td>
    </tr>
  </tbody>
</table>

<p>基于参数修剪和共享的方法针对模型参数的冗余性，试图去除冗余和不重要的项。基于低秩因子分解的技术使用矩阵/张量分解来估计深度学习模型的信息参数。基于传输/紧凑卷积滤波器的方法设计了特殊的结构卷积滤</p>

<p>波器来降低存储和计算复杂度。知识蒸馏方法通过学习一个蒸馏模型，训练一个更紧凑的神经网络来重现一个更大的网络的输出。</p>

<p>一般来说，参数修剪和共享，低秩分解和知识蒸馏方法可以用于全连接层和卷积层的CNN，但另一方面，使用转移/紧凑型卷积核的方法仅支持卷积层。低秩因子分解和基于转换/紧凑型卷积核的方法提供了一个端到端的</p>

<p>流水线，可以很容易地在CPU/GPU环境中实现。相反参数修剪和共享使用不同的方法，如矢量量化，二进制编码和稀疏约束来执行任务，这导致常需要几个步骤才能达到目标。</p>

<h2 id="移动端部署">移动端部署</h2>

<p>目前，很多公司都推出了开源的移动端深度学习框架，基本不支持训练，只支持前向推理。这些框架都是 offline 方式，它可确保用户数据的私有性，可不再依赖于因特网连接。</p>

<h3 id="caffe2">Caffe2</h3>
<p>2017年4月19日 Facebook在F8开发者大会上推出Caffe2。项目是专门为手机定制的深度框架，是在caffe2 的基础上进行迁移的，目的就是让最普遍的智能设备——手机也能广泛高效地应用深度学习算法。</p>

<p>Caffe2 is a deep learning framework made with expression, speed, and modularity in mind. It is an experimental refactoring of Caffe, and allows a more flexible</p>

<p>way to organize computation.</p>

<p>开源地址: https://github.com/caffe2/caffe2</p>

<h3 id="tensorflow-lite">TensorFlow Lite</h3>
<p>2017年5月17日 Goole在I/O大会推出TensorFlow Lite，是专门为移动设备而优化的 TensorFlow 版本。TensorFlow Lite 具备以下三个重要功能：</p>

<p>轻量级（Lightweight）：支持机器学习模型的推理在较小二进制数下进行，能快速初始化/启动</p>

<p>跨平台（Cross-platform）：可以在许多不同的平台上运行，现在支持 Android 和 iOS</p>

<p>快速（Fast）：针对移动设备进行了优化，包括大大减少了模型加载时间、支持硬件加速</p>

<p>模块如下:</p>

<p>TensorFlow Model: 存储在硬盘上已经训练好的 TensorFlow 模型</p>

<p>TensorFlow Lite Converter: 将模型转换为 TensorFlow Lite 文件格式的程序</p>

<p>TensorFlow Lite Model File: 基于 FlatBuffers 的模型文件格式，针对速度和大小进行了优化。</p>

<p>TensorFlow Lite 目前支持很多针对移动端训练和优化好的模型。</p>

<h3 id="core-ml">Core ML</h3>
<p>2017年6月6日 Apple在WWDC大会上推出Core ML。对机器学习模型的训练是一项很重的工作，Core ML 所扮演的角色更多的是将已经训练好的模型转换为 iOS 可以理解的形式，并且将新的数据“喂给”模型，获取</p>

<p>输出。抽象问题和创建模型虽然并不难，但是对模型的改进和训练可以说是值得研究一辈子的事情，这篇文章的读者可能也不太会对此感冒。好在 Apple 提供了一系列的工具用来将各类机器学习模型转换为 Core</p>

<p>ML 可以理解的形式。籍此，你就可以轻松地在你的 iOS app 里使用前人训练出的模型。这在以前可能会需要你自己去寻找模型，然后写一些 C++ 的代码来跨平台调用，而且难以利用 iOS 设备的 GPU 性能和</p>

<p>Metal (除非你自己写一些 shader 来进行矩阵运算)。Core ML 将使用模型的门槛降低了很多。</p>

<p>Core ML 在背后驱动了 iOS 的视觉识别的 Vision 框架和 Foundation 中的语义分析相关 API。普通开发者可以从这些高层的 API 中直接获益，比如人脸图片或者文字识别等。这部分内容在以前版本的 SDK</p>

<p>中也存在，不过在 iOS 11 SDK 中它们被集中到了新的框架中，并将一些更具体和底层的控制开放出来。比如你可以使用 Vision 中的高层接口，但是同时指定底层所使用的模型。这给 iOS 的计算机视觉带来了新</p>

<p>的可能。</p>

<h3 id="mace">MACE</h3>
<p>小米开源了深度学习框架MACE，有几个特点：异构加速、汇编级优化、支持各种框架的模型转换。</p>

<p>有了异构，就可以在CPU、GPU和DSP上跑不同的模型，实现真正的生产部署，比如人脸检测、人脸识别和人脸跟踪，可以同时跑在不同的硬件上。小米支持的GPU不限于高通，这点很通用，很好，比如瑞芯微的RK3299</p>

<p>就可以同时发挥出cpu和GPU的好处来。</p>

<p>MACE 是专门为移动设备优化的深度学习模型预测框架，MACE 从设计之初，便针对移动设备的特点进行了专门的优化：</p>

<p>速度：对于放在移动端进行计算的模型，一般对整体的预测延迟有着非常高的要求。在框架底层，针对ARM CPU进行了NEON指令级优化，针对移动端GPU，实现了高效的OpenCL内核代码。针对高通DSP，集成了nnlib</p>

<p>计算库进行HVX加速。同时在算法层面，采用Winograd算法对卷积进行加速。</p>

<p>功耗：移动端对功耗非常敏感，框架针对ARM处理器的big.LITTLE架构，提供了高性能，低功耗等多种组合配置。针对Adreno GPU，提供了不同的功耗性能选项，使得开发者能够对性能和功耗进行灵活的调整。</p>

<p>系统响应：对于GPU计算模式，框架底层对OpenCL内核自适应的进行分拆调度，保证GPU渲染任务能够更好的进行抢占调度，从而保证系统的流畅度。</p>

<p>初始化延迟：在实际项目中，初始化时间对用户体验至关重要，框架对此进行了针对性的优化。</p>

<p>内存占用：通过对模型的算子进行依赖分析，引入内存复用技术，大大减少了内存的占用。</p>

<p>模型保护：对于移动端模型，知识产权的保护往往非常重要，MACE支持将模型转换成C++代码，大大提高了逆向工程的难度。</p>

<p>此外，MACE 支持 TensorFlow 和 Caffe 模型，提供转换工具，可以将训练好的模型转换成专有的模型数据文件，同时还可以选择将模型转换成C++代码，支持生成动态库或者静态库，提高模型保密性。</p>

<p>目前MACE已经在小米手机上的多个应用场景得到了应用，其中包括相机的人像模式，场景识别，图像超分辨率，离线翻译(即将实现)等。</p>

<p>开源地址：https://github.com/XiaoMi/mace</p>

<p>MACE Model Zoo
随着MACE一起开源的还有MACE Model Zoo项目，目前包含了物体识别，场景语义分割，图像风格化等多个公开模型。</p>

<p>链接: https://github.com/XiaoMi/mace-models</p>

<h3 id="feathercnn-和-ncnn">FeatherCNN 和 NCNN</h3>
<p>FeatherCNN 由腾讯 AI 平台部研发，基于 ARM 架构开发的高效神经网络前向计算库，核心算法已申请专利。该计算库支持 caffe 模型，具有无依赖，速度快，轻量级三大特性。该库具有以下特性：</p>

<p>无依赖：该计算库无第三方组件，静态库或者源码可轻松部署于 ARM 服务器，和嵌入式终端，安卓，苹果手机等移动智能设备。</p>

<p>速度快：该计算库是当前性能最好的开源前向计算库之一，在 64 核 ARM 众核芯片上比 Caffe 和 Caffe2 快 6 倍和 12 倍，在 iPhone7 上比 Tensorflow lite 快 2.5 倍。</p>

<p>轻量级：该计算库编译后的后端 Linux 静态库仅 115KB , 前端 Linux 静态库 575KB , 可执行文件仅 246KB 。</p>

<p>FeatherCNN 采用 TensorGEMM 加速的 Winograd 变种算法，以 ARM 指令集极致提升 CPU 效率，为移动端提供强大的 AI 计算能力。使用该计算库可接近甚至达到专业神经网络芯片或 GPU 的性能，并保护用户已有硬件投资。</p>

<p>NCNN 是一个为手机端极致优化的高性能神经网络前向计算框架。ncnn 从设计之初深刻考虑手机端的部署和使用。无第三方依赖，跨平台，手机端 cpu 的速度快于目前所有已知的开源框架。基于 ncnn，开发者能够</p>

<p>将深度学习算法轻松移植到手机端高效执行，开发出人工智能 APP，将 AI 带到你的指尖。ncnn 目前已在腾讯多款应用中使用，如QQ，Qzone，微信，天天P图等。</p>

<p>这两个框架都是腾讯公司出品，FeatherCNN来自腾讯AI平台部，NCNN来自腾讯优图。</p>

<p>重点是：都开源，都只支持cpu</p>

<p>NCNN开源早点，性能较好，用户较多。FeatherCNN开源晚，底子很好。</p>

<p>FeatherCNN开源地址：http://github.com/tencent/FeatherCNN</p>

<p>NCNN开源地址：https://github.com/Tencent/ncnn</p>

<h3 id="mdl">MDL</h3>
<p>百度的mobile-deep-learning，MDL 框架主要包括模型转换模块（MDL Converter）、模型加载模块（Loader）、网络管理模块（Net）、矩阵运算模块（Gemmers）及供 Android 端调用的 JNI 接口层</p>

<p>（JNI Interfaces）。其中，模型转换模块主要负责将 Caffe 模型转为 MDL 模型，同时支持将 32bit 浮点型参数量化为 8bit 参数，从而极大地压缩模型体积；模型加载模块主要完成模型的反量化及加载校</p>

<p>验、网络注册等过程，网络管理模块主要负责网络中各层 Layer 的初始化及管理工作；MDL 提供了供 Android 端调用的 JNI 接口层，开发者可以通过调用 JNI 接口轻松完成加载及预测过程。</p>

<p>作为一款移动端深度学习框架，需要充分考虑到移动应用自身及运行环境的特点，在速度、体积、资源占用率等方面有严格的要求。同时，可扩展性、鲁棒性、兼容性也是需要考虑的。为了保证框架的可扩展性，MDL</p>

<p>对 layer 层进行了抽象，方便框架使用者根据模型的需要，自定义实现特定类型的层，使用 MDL 通过添加不同类型的层实现对更多网络模型的支持，而不需要改动其他位置的代码。为了保证框架的鲁棒性，MDL 通</p>

<p>过反射机制，将 C++ 底层异常抛到应用层，应用层通过捕获异常对异常进行相应处理，如通过日志收集异常信息、保证软件可持续优化等。目前行业内各种深度学习训练框架种类繁多，而 MDL 不支持模型训练能力，</p>

<p>为了保证框架的兼容性，MDL提供 Caffe 模型转 MDL 的工具脚本，使用者通过一行命令就可以完成模型的转换及量化过程。</p>

<p>开源地址：https://github.com/baidu/mobile-deep-learning</p>

<h3 id="snpe">SNPE</h3>
<p>这是高通骁龙的官方SDK，不开源。主要支持自家的DSP、GPU和CPU。模型训练在流行的深度学习框架上进行（SNPE支持Caffe，Caffe2，ONNX和TensorFlow模型。）训练完成后，训练的模型将转换为可加载到</p>

<p>SNPE运行时的DLC文件。 然后，可以使用此DLC文件使用其中一个Snapdragon加速计算核心执行前向推断传递。</p>

<h2 id="实践应用">实践应用</h2>

<p>以智云视图的人脸标定算法为例，开源项目地址为：</p>

<p>https://github.com/zeusees/HyperLandmark</p>

<p>使用的是腾讯的 NCNN 进行部署，首先通过 caffe 或者 tensorflow 等进行训练，例如使用 caffe 训练会生成模型：</p>

<p>deploy.prototxt</p>

<p>snapshot_10000.caffemodel</p>

<p>然后，使用 caffe 自带了工具可以把老版本的 caffe 网络和模型转换为新版（ncnn的工具只认识新版）</p>

<p>upgrade_net_proto_text [老prototxt] [新prototxt]</p>

<p>upgrade_net_proto_binary [老caffemodel] [新caffemodel]</p>

<p>使用 caffe2ncnn 工具转换为 ncnn 的网络描述和模型</p>

<p>caffe2ncnn deploy.prototxt bvlc_alexnet.caffemodel alexnet.param alexnet.bin</p>

<p>但是 param 描述文件是明文的，可以使用 ncnn2mem 工具转换为二进制描述文件和内存模型，生成 alexnet.param.bin 和两个静态数组的代码文件</p>

<p>然后可以使用 NCNN 加载二进制的 param.bin 和 bin 模型</p>

<p>ncnn::Net net;</p>

<p>net.load_param_bin(“alexnet.param.bin”);</p>

<p>net.load_model(“alexnet.bin”);</p>

<p>获取输入并计算输出结果</p>

<p>ncnn 用自己的数据结构 Mat 来存放输入和输出数据 输入图像的数据要转换为 Mat，依需要减去均值和乘系数</p>

<p>#include “mat.h”</p>

<p>unsigned char* rgbdata;// data pointer to RGB image pixels</p>

<p>int w;// image width</p>

<p>int h;// image height</p>

<p>ncnn::Mat in = ncnn::Mat::from_pixels(rgbdata, ncnn::Mat::PIXEL_RGB, w, h);</p>

<p>const float mean_vals[3] = {104.f, 117.f, 123.f};</p>

<p>in.substract_mean_normalize(mean_vals, 0);</p>

<p>执行前向网络，获得计算结果</p>

<p>#include “net.h”</p>

<p>#include “alexnet.id.h”</p>

<p>ncnn::Mat in;// input blob as above</p>

<p>ncnn::Mat out;</p>

<p>ncnn::Extractor ex = net.create_extractor();</p>

<p>ex.set_light_mode(true);</p>

<p>ex.input(alexnet_param_id::BLOB_data, in);</p>

<p>ex.extract(alexnet_param_id::BLOB_prob, out);</p>

:ET