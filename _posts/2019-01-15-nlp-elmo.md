---
layout: post
title:  "Deep contextualized word representations(ELMO)"
date: 2019-01-16 05:30:00
categories: nlp
tags: [word representations]
---
<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>


近年来，研究人员通过文本上下文信息分析获得更好的词向量。ELMo是其中的翘楚，在多个任务、多个数据集上都有显著的提升。所以，它是目前最好用的词向量，the-state-of-the-art的方法。这篇文章发表在2018年的NAACL上，outstanding paper award下面就简单介绍一下这个“神秘”的词向量模型。<!-- more -->

##  1、Introduction

* **什么是一个好的词向量**

    + 能够反映出语义和语法的复杂特征.

    + 能够准确的对不同上下文进行反应.


* **deep contextualized 词向量的特点**
    * **使用理念方面:**
        + 在原先的词向量模型中, 每个词对应着一个向量, 但是这个模型是根据一个句子赋予每个词汇向量. 因此对于一个n-tokens的输入NLP任务模型, 输入到NLP任务模型的是n个向量. 这个论文中提出的方法, 是在NLP模型的输入之前需要再加一个动态计算词向量的前向网络, 我们称其为BiLMs, 而这个前向网络是提前在一个大的数据集上训练好的. 而这个前向网络的输入是n个更初始的词向量.
    
    * **训练具体信息**:
        + 简单来说是一个多层的双向LSTMs结构, 称其为BiLMs模型.
    
    * **效果方面:**
        + BiLMs的较高层次的隐藏单元的向量很好的抓住了词汇的意义表示, 并且在很多任务上不用再次对词向量进行调整就可以得到一个不错的结果.        
        + BiLMs的较低层次的隐藏单元的向量很好的抓住了词汇的语法信息, 可以在例如词性标注的任务上发挥其作用.        
        + 这种深度模型所带来的分层效果使得将一套词向量应用于不同任务有了可能性. 因为, 每个任务所需要的信息多是不同的.在六个任务上进行了测试,都发现好于最佳结果.


## 2、ELMo: Embeddings from Language Models

### 2.1 Bidirectional language model

语言模型就是生成文本的方式、方法，是多个N个词语的序列（t<sub>1</sub>, t<sub>2</sub>,...,t<sub>N</sub>)的极大似然。

<!-- 最近，如《Exploring the limits of language modeling》、《On the state of the art of evaluation in neural language models》和《Regularizing and optimizing lstm language models》等论文
中，首先使用character-level的RNN或CNN，计算得到“上下文无关”（context-independent）词向量表示
<img src="http://chart.googleapis.com/chart?cht=tx&chl=$${x_{k}}^{LM}$$" style="border:none;">，然后将此向量feed进入L层的前向LSTM。在每一个位置k ，每个LSTM层会输出一个
<img src="http://chart.googleapis.com/chart?cht=tx&chl=\vec{h}_{k,j}^{LM}" style="border:none;">，其中j=1,....L. 最顶层的LSTM输出为
<img src="http://chart.googleapis.com/chart?cht=tx&chl=\vec{h}_{k,L}^{LM}" style="border:none;">
  ，然后加上softmax来预测下一个词语 t<sub>k+1</sub> 。 -->

前向语言模型就是，已知（t<sub>1</sub>, t<sub>2</sub>,...,t<sub>k-1</sub>)，预测下一个词语t<sub>k</sub>的概率，写成公式就是

<img src="http://chart.googleapis.com/chart?cht=tx&chl=$$p(t_1,t_2,...,t_N)=\prod_{k=1}^{N}p(t_k|t_1,t_2,...,t_{k-1})$$" style="border:none;">

既然是双向，后向的语言模型如下，即通过下文预测之前的词语：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=$$p(t_1,t_2,...,t_N)=\prod_{k=1}^{N}p(t_k|t_{k%2B1},t_{k%2B2},...,t_N)$$" style="border:none;">

双向语言模型（biLM）将前后向语言模型结合起来，最大化前向、后向模型的联合似然函数即可，如下式所示：


<img src="http://chart.googleapis.com/chart?cht=tx&chl=$$\sum_{k=1}^{N}{\left(logp(t_k|t_1,t_2...t_{k-1};Q_x,\underset{Q_{LSTM}}{\rightarrow},Q_s)%2Blogp(t_k|t_{k%2B1},t_{k%2B2}...t_{N};Q_x,\underset{Q_{LSTM}}{\leftarrow},Q_s)\right)}$$" style="border:none;">

其中Q<sub>x</sub> 和 Q<sub>s</sub>分别是context-independent词向量训练时和softmax层的参数,他们是相同的，<img src="http://chart.googleapis.com/chart?cht=tx&chl=\underset{Q_{LSTM}}{\rightarrow}" style="border:none;">和 
<img src="http://chart.googleapis.com/chart?cht=tx&chl=\underset{Q_{LSTM}}{\leftarrow}" style="border:none;">则是双向语言模型的（前后向语言模型的）参数。单层双向lstm如下：
<img src='/imgs/elmo/elmo_bilstm.jpg'>

<br/>
那么, 这里设定第 j 层的第 k 个位置的forward LSTM(也就是右向LSTM)输出为<img src="http://chart.googleapis.com/chart?cht=tx&chl=\underset{h_{k,j}^{LM}}{\rightarrow}" style="border:none;">

那么, 左向就是<img src="http://chart.googleapis.com/chart?cht=tx&chl=\underset{h_{k,j}^{LM}}{\leftarrow}" style="border:none;">

Input, 也就是最原始的词向量为<img src="http://chart.googleapis.com/chart?cht=tx&chl=$${x_{k}}^{LM}$$" style="border:none;">

那么, 这个最原始的词向量是怎么获得的呢?前面提到了, 是通过字符卷积, 大概是这个样子:

<div align="center"><img src='/imgs/elmo/elmo_embedding_network.jpg'  style='align:center'></div>



### 2.2 Elmo


上面说到了, LSTM的每一层都有自己不同的代表的意义, 因此,对于不同的任务, 每层参与表征的权重也就不同, 因此, 为了一般化, 该方案设置一个通用词向量表征法, 即利用每层状态的线性组合.

<img src='/imgs/elmo/elmo_rep.jpg'>

这是线性组合的单元集合.

<img src='/imgs/elmo/elmo_elmo.jpg'>

其中,s就是softmax-normalized weights , 也就是加和为1的一组权重向量. 这组权重不仅仅代表的是权重, 因为每层LSTM内部状态,也就是 h 的分布是不同的, 这个也可以用作layer normalization.

<img src="http://chart.googleapis.com/chart?cht=tx&chl=\gamma" style="border:none;"> 允许具体的task模型去放缩 ELMo 的大小. 这个地方非常重要, 作者在补充中进行了强调, 在没有这个的情况下, 只采用该模型的最后一层输出作为词向量的时候的效果甚至差于 baseline. 这个重要的原因是, BiLM的内部表征和具体任务的表征的分布是不一样的.


### 2.3 Using biLMs for supervised NLP tasks


* **最简版使用方法**

    将该词向量加到一般的任务的方法是, 先固定一个权重, 构成一个 $ELMO_t^{task}$ ​ , 然后联结 ​ $ELMO_t^{task}$ 和 最初始词向量(也就是通过字符卷积获得的向量) 为 [​ $x_k$;$ELMO_t^{task}$], 然后将这个向量输入到任务的RNN模型中去. 然后在训练中一同训练权重因子.
* **加强版**
  
    我们可以在output处也加一个这样的向量,即 ​ $[h_k;ELMO_t^{task}]$ , 不同是,这里应该采用与输入 $ELMO_t^{task}$ 中不同的权重因子.

* **终极版**
  
    可以在ELMo模型中加入dropout, 以及采用 L2 loss的方法来提升模型.
    并且, 这里的 L2 项的系数 ​ $\lambda$ 越大, 越有取各层平均值的意思, 越小, 越可以发挥各个层之间的不同带来的效果. 但是并不是说, 越小越好

### 2.4 Pre-trained bidirectional language model architecture

现在进入真正重要的部分, 也就是如何训练这个 ELMo 网络的问题.

* **模型结构**
  
    这里采用的还是语言模型去训练网络. 并且这篇的模型是有借鉴先行研究的. 

    Exploring the limits of language modeling.

    Character-Aware Neural Language Models

    这两个先行研究都是基于字符卷积网络来做的, 但是这其中有一个问题是字符卷积网络的低效问题, 详细见论文. 通过这篇论文我们也知道, 能够实现最大效率利用字符共现来表达词义的网络就是biLSTMs 网络, 这也是为什么这个模型采用这个网络的原因.

    但是不同于这两个论文的是, 这个论文增加了

    * 使得模型可以同时训练双向参数的

    * 增加 LSTMs单元之间的残差连接(残差网络). 简单说, 就是一种简化训练,提升在深层网络下训练效率的方法.
  
    并且, 这里使用的是 CNN-BIG-LSTM , 也就是说模型的 size 特别大. 这个主要是从 Exploring the limits of language modeling 中学得的.


* **模型参数**

    最终的模型参数如下, 这些参数的示意都在 (Exploring the limits of language modeling) 这篇文章中,由于篇幅较长, 见最后的补充部分.

    * 是一个两层的LSTM, 每层的LSTM cell拥有4096个单元(即hidden state) 和512维度映射
     
    * 从第一层到第二层存在一个残差连接
     
    * context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers. 也就是说使用了2048个过滤器, 来进行基于字符的卷积计算.
     
    * a linear projection down to a 512 representation. 也就是一直在说的各层输出的线性组合.
      
    最后的结果是, 整个模型针对一个token, 可以产生三个向量, 原始 , 第一层以及第二层.

### 2.5 Evaluation

论文从 Question answering, Textual entailment, Semantic role labeling, Coreference resolution, Named entity extraction, Sentiment analysis 六个人物来验证, 都取得了提升.

## 3、 Supplementary

这里主要补充被该论文一概而过, 但是又极其重要的部分.主要基于两篇论文:Character-Aware Neural Language Models 和 Exploring the Limits of Language Modeling，具体可查看论文


