---
layout: post
title:  "Deep contextualized word representations(ELMO)"
date: 2019-01-16 05:30:00
categories: nlp
tags: [word representations]
---

近年来，研究人员通过文本上下文信息分析获得更好的词向量。ELMo是其中的翘楚，在多个任务、多个数据集上都有显著的提升。所以，它是目前最好用的词向量，the-state-of-the-art的方法。这篇文章发表在2018年的NAACL上，outstanding paper award下面就简单介绍一下这个“神秘”的词向量模型。<!-- more -->

##  Introduction

* **什么是一个好的词向量**

    + 能够反映出语义和语法的复杂特征.

    + 能够准确的对不同上下文进行反应.


* **deep contextualized 词向量的特点**
    * **使用理念方面:**
        + 在原先的词向量模型中, 每个词对应着一个向量, 但是这个模型是根据一个句子赋予每个词汇向量. 因此对于一个 n-tokens 的输入NLP任务模型, 输入到NLP任务模型的是n个向量. 这个论文中提出的方法, 是在NLP模型的输入之前需要再加一个动态计算词向量的前向网络, 我们称其为BiLMs, 而这个前向网络是提前在一个大的数据集上训练好的. 而这个前向网络的输入是n个更初始的词向量.
    
    * **训练具体信息**:
        + 简单来说是一个多层的双向LSTMs结构, 称其为BiLMs模型.
    
    * **效果方面:**
        + BiLMs的较高层次的隐藏单元的向量很好的抓住了词汇的意义表示, 并且在很多任务上不用再次对词向量进行调整就可以得到一个不错的结果.        
        + BiLMs的较低层次的隐藏单元的向量很好的抓住了词汇的语法信息, 可以在例如词性标注的任务上发挥其作用.        
        + 这种深度模型所带来的分层效果使得将一套词向量应用于不同任务有了可能性. 因为, 每个任务所需要的信息多是不同的.在六个任务上进行了测试,都发现好于最佳结果.



## Bidirectional language model

语言模型就是生成文本的方式、方法，是多个N个词语的序列（t<sub>1</sub>, t<sub>2</sub>,...,t<sub>N</sub>)的极大似然。前向语言模型就是，已知（t<sub>1</sub>, t<sub>2</sub>,...,t<sub>k-1</sub>)，预测下一个词语t<sub>k</sub>的概率，写成公式就是

<img src="http://chart.googleapis.com/chart?cht=tx&chl= $$p(t_1, t_2,...,t_N)=\prod_{k=1}^{N}p(t_k|t_1, t_2,...,t_{k-1})$$" style="border:none;">

最近，如《Exploring the limits of language modeling》、《On the state of the art of evaluation in neural language models》和《Regularizing and optimizing lstm language models》等论文
中，首先使用character-level的RNN或CNN，计算得到“上下文无关”（context-independent）词向量表示
<img src="http://chart.googleapis.com/chart?cht=tx&chl= {x_{k}}^{LM}" style="border:none;">，然后将此向量feed进入L层的前向LSTM。在每一个位置 k ，每个LSTM层会输出一个
<img src="http://chart.googleapis.com/chart?cht=tx&chl= \vec{x}_{k,j}^{LM}" style="border:none;">，其中j=1,....L. 最顶层的LSTM输出为
<img src="http://chart.googleapis.com/chart?cht=tx&chl= \vec{x}_{k,L}^{LM}" style="border:none;">
  ，然后加上softmax来预测下一个词语 t<sub>k+1</sub> 。
既然是双向，后向的语言模型如下，即通过下文预测之前的词语：

<img src="http://chart.googleapis.com/chart?cht=tx&chl= $$p(t_1,t_2,...,t_N)=\prod_{k=1}^{N}p(t_k|t_{k+1},t_{k+2},...,t_N)$$" style="border:none;">


 

双向语言模型（biLM）将前后向语言模型结合起来，最大化前向、后向模型的联合似然函数即可，如下式所示：


<img src="http://chart.googleapis.com/chart?cht=tx&chl= \sum_{k=1}^{N}{\left ( log\ p(t_k|t_1,t_2,...,t_{k-1}; \Theta, \overrightarrow{\Theta}_{LSTM},\Theta_s)+log\ p(t_k|t_{k+1},t_{k+2},...,t_{N}; \Theta, \overleftarrow{\Theta}_{LSTM},\Theta_s)\right ) }." style="border:none;">

其中， \Theta_x 和 \Theta_s 分别是context-independent词向量训练时 和 soft max层的参数， \overrightarrow{\Theta}_{LSTM} 和 \overleftarrow{\Theta}_{LSTM}则是双向语言模型的（前后向语言模型的）参数。

